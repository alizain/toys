# Session Summary: Bradley-Terry Rating Algorithm Validation

**Session ID**: 9eed1f77-7977-4b35-a5ba-c341ceaea226

**Date/Timestamp**: 2025-12-26, 14:19 - 14:28 UTC

**Main Topic**: Validating and refining the Bradley-Terry model implementation in `compute-ratings.ts` for toy rating calculations.

---

## What Was Tried

1. **Deep dive into Bradley-Terry algorithm logic**: Analyzed the `computeBradleyTerryScores()` function to understand how pairwise comparisons are converted to ratings across 7 dimensions (generativity, developmental_longevity, productive_challenge, sensory_engagement, expressive_range, social_affordance, practical_sustainability).

2. **Added duplicate comparison validation**: Implemented a runtime check to verify the assumption that each toy pair is only compared once per dimension. Added error throwing when duplicates are detected.

3. **Analyzed two normalization steps**:
   - Per-iteration normalization (line ~195): Divides scores by average to keep centered around 1.0
   - Final normalization (line ~206): Linear mapping to 1-10 scale

4. **Tested removing per-iteration normalization**: Removed the per-iteration normalization to verify if it was truly necessary. Ran the script and compared outputs.

---

## What Was Found/Learned

1. **Duplicate validation works**: Successfully added guard against duplicate toy comparisons per dimension.

2. **Per-iteration normalization IS necessary**: Without it, Bradley-Terry scores exhibit extreme scale separation:
   - Winners: scores of 3000-7000
   - Losers: scores approaching 0
   - When linearly normalized to [1,10], nearly all toys compress to ~1

3. **The normalization comment was misleading**: The original comment said "prevent drift" for convergence. The real purpose is to prevent extreme scale separation that breaks subsequent linear normalization.

4. **Incomplete data amplifies the problem**: With only 10 of 19 toys having comparison data, some toys have only wins or only losses, exacerbating the scale separation issue.

---

## What Was Accomplished

- **Added duplicate comparison detection** in `computeStats()` function (lines 128-132)
- **Validated the necessity of per-iteration normalization** through empirical testing
- **Identified need to update the comment** explaining the normalization's true purpose (preventing scale separation, not just numerical drift)
- **Left code with debug output** for score range visibility (to be cleaned up)

---

## Key Quotes/Thoughts

- "Bradley-Terry parameters are only identifiable up to a multiplicative constant" - the scale is arbitrary, only ratios matter
- "The normalization IS necessary, but the comment is misleading. It's not about 'preventing drift' for Bradley-Terry convergence - it's about preventing extreme scale separation that breaks the subsequent linear normalization."
- On the diff between normalized and non-normalized runs: "Without normalization, many toys collapse to score 1"

---

## Open Questions

1. Once all pairwise comparisons are complete, will the per-iteration normalization still be necessary? With complete data, every toy will have both wins and losses, potentially keeping scores more bounded naturally.

2. Should unrated toys (those with no comparison data) be handled differently - perhaps excluded from min/max calculations or given the median score instead of 1.0?

3. Would log-scale normalization be more appropriate than linear for handling score distributions?

---

## Files Modified

- `/Users/alizain/803/web/packages/toys/scripts/compute-ratings.ts`
  - Added duplicate comparison detection
  - Temporarily added debug logging for BT score ranges
  - Per-iteration normalization was temporarily removed then discussion paused before restoration
